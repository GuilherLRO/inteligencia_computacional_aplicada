# Parte 1: An√°lise Explorat√≥ria Completa dos Dados

## üìä O que √© An√°lise Explorat√≥ria de Dados (EDA)?

A An√°lise Explorat√≥ria de Dados √© o primeiro passo fundamental em qualquer projeto de machine learning. Ela nos permite:
- **Entender** a estrutura dos dados
- **Identificar** problemas nos dados
- **Descobrir** padr√µes e relacionamentos
- **Tomar decis√µes** sobre pr√©-processamento

## üöÄ Passo a Passo da Implementa√ß√£o

### 1. Importa√ß√£o das Bibliotecas Necess√°rias

```python
# Bibliotecas para manipula√ß√£o e an√°lise de dados
import pandas as pd
import numpy as np

# Bibliotecas para visualiza√ß√£o
import matplotlib.pyplot as plt
import seaborn as sns

# Configura√ß√µes para melhor visualiza√ß√£o
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 12

# Para exibir todas as colunas do DataFrame
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
```

### 2. Carregamento e Primeira Visualiza√ß√£o dos Dados

```python
# Carregar o dataset
df = pd.read_excel('Real estate valuation data set.xlsx')

# Visualizar as primeiras linhas
print("=== PRIMEIRAS 5 LINHAS ===")
print(df.head())

# Visualizar as √∫ltimas linhas
print("\n=== √öLTIMAS 5 LINHAS ===")
print(df.tail())

# Informa√ß√µes b√°sicas sobre o dataset
print("\n=== INFORMA√á√ïES B√ÅSICAS ===")
print(f"Forma do dataset: {df.shape}")
print(f"N√∫mero de linhas: {df.shape[0]}")
print(f"N√∫mero de colunas: {df.shape[1]}")
```

### 3. An√°lise da Estrutura dos Dados

```python
# Informa√ß√µes detalhadas sobre as colunas
print("=== INFORMA√á√ïES DETALHADAS DAS COLUNAS ===")
print(df.info())

# Tipos de dados de cada coluna
print("\n=== TIPOS DE DADOS ===")
print(df.dtypes)

# Nomes das colunas
print("\n=== NOMES DAS COLUNAS ===")
for i, col in enumerate(df.columns):
    print(f"{i+1}. {col}")
```

### 4. Estat√≠sticas Descritivas

```python
# Estat√≠sticas descritivas b√°sicas
print("=== ESTAT√çSTICAS DESCRITIVAS ===")
print(df.describe())

# Estat√≠sticas descritivas incluindo vari√°veis categ√≥ricas
print("\n=== ESTAT√çSTICAS COMPLETAS ===")
print(df.describe(include='all'))

# Verificar se h√° valores √∫nicos em cada coluna
print("\n=== VALORES √öNICOS POR COLUNA ===")
for col in df.columns:
    unique_count = df[col].nunique()
    print(f"{col}: {unique_count} valores √∫nicos")
```

### 5. Verifica√ß√£o de Valores Nulos/Missing

```python
# Verificar valores nulos
print("=== VERIFICA√á√ÉO DE VALORES NULOS ===")
null_counts = df.isnull().sum()
print("Valores nulos por coluna:")
print(null_counts)

# Porcentagem de valores nulos
print("\n=== PERCENTUAL DE VALORES NULOS ===")
null_percentages = (df.isnull().sum() / len(df)) * 100
for col, percentage in null_percentages.items():
    if percentage > 0:
        print(f"{col}: {percentage:.2f}%")
    else:
        print(f"{col}: 0% (sem valores nulos)")

# Visualiza√ß√£o gr√°fica dos valores nulos
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis')
plt.title('Mapa de Valores Nulos no Dataset')
plt.tight_layout()
plt.show()
```

### 6. An√°lise das Vari√°veis Individuais

#### 6.1 Vari√°veis Num√©ricas
```python
# Selecionar apenas colunas num√©ricas
numeric_columns = df.select_dtypes(include=[np.number]).columns
print(f"Colunas num√©ricas: {list(numeric_columns)}")

# Histogramas para vari√°veis num√©ricas
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.ravel()

for i, col in enumerate(numeric_columns):
    if i < len(axes):
        axes[i].hist(df[col], bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        axes[i].set_title(f'Distribui√ß√£o de {col}')
        axes[i].set_xlabel(col)
        axes[i].set_ylabel('Frequ√™ncia')
        axes[i].grid(True, alpha=0.3)

# Remover subplots vazios se houver
for i in range(len(numeric_columns), len(axes)):
    fig.delaxes(axes[i])

plt.tight_layout()
plt.show()

# Boxplots para identificar outliers
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.ravel()

for i, col in enumerate(numeric_columns):
    if i < len(axes):
        axes[i].boxplot(df[col])
        axes[i].set_title(f'Boxplot de {col}')
        axes[i].set_ylabel(col)
        axes[i].grid(True, alpha=0.3)

# Remover subplots vazios se houver
for i in range(len(numeric_columns), len(axes)):
    fig.delaxes(axes[i])

plt.tight_layout()
plt.show()
```

#### 6.2 An√°lise de Outliers
```python
# Identificar outliers usando IQR (Interquartile Range)
print("=== AN√ÅLISE DE OUTLIERS (M√©todo IQR) ===")
for col in numeric_columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    
    print(f"\n{col}:")
    print(f"  Q1: {Q1:.2f}")
    print(f"  Q3: {Q3:.2f}")
    print(f"  IQR: {IQR:.2f}")
    print(f"  Limite inferior: {lower_bound:.2f}")
    print(f"  Limite superior: {upper_bound:.2f}")
    print(f"  N√∫mero de outliers: {len(outliers)}")
    print(f"  Percentual de outliers: {(len(outliers)/len(df)*100):.2f}%")
```

### 7. An√°lise de Correla√ß√µes

```python
# Matriz de correla√ß√£o
print("=== MATRIZ DE CORRELA√á√ÉO ===")
correlation_matrix = df[numeric_columns].corr()
print(correlation_matrix)

# Visualiza√ß√£o da matriz de correla√ß√£o
plt.figure(figsize=(10, 8))
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
sns.heatmap(correlation_matrix, 
            mask=mask,
            annot=True, 
            cmap='coolwarm', 
            center=0,
            square=True,
            fmt='.3f')
plt.title('Matriz de Correla√ß√£o entre Vari√°veis Num√©ricas')
plt.tight_layout()
plt.show()

# Correla√ß√µes com a vari√°vel target (assumindo que √© a √∫ltima coluna)
target_col = df.columns[-1]
print(f"\n=== CORRELA√á√ïES COM A VARI√ÅVEL TARGET ({target_col}) ===")
target_correlations = correlation_matrix[target_col].sort_values(ascending=False)
print(target_correlations)
```

### 8. An√°lise de Distribui√ß√µes Conjuntas

```python
# Scatter plots para vari√°veis mais correlacionadas com o target
top_correlations = target_correlations[1:4]  # Top 3 correla√ß√µes (excluindo a pr√≥pria vari√°vel)
print(f"Vari√°veis com maior correla√ß√£o com {target_col}:")
print(top_correlations)

# Criar scatter plots
fig, axes = plt.subplots(1, 3, figsize=(18, 6))
for i, (col, corr) in enumerate(top_correlations.items()):
    axes[i].scatter(df[col], df[target_col], alpha=0.6, color='blue')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel(target_col)
    axes[i].set_title(f'{col} vs {target_col}\nCorrela√ß√£o: {corr:.3f}')
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### 9. Resumo da An√°lise Explorat√≥ria

```python
# Criar um resumo executivo
print("=== RESUMO EXECUTIVO DA AN√ÅLISE EXPLORAT√ìRIA ===")
print(f"Dataset: Real Estate Valuation")
print(f"Dimens√µes: {df.shape[0]} linhas √ó {df.shape[1]} colunas")
print(f"Vari√°veis num√©ricas: {len(numeric_columns)}")
print(f"Vari√°veis categ√≥ricas: {len(df.columns) - len(numeric_columns)}")
print(f"Valores nulos: {df.isnull().sum().sum()}")
print(f"Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

print(f"\nVari√°vel target: {target_col}")
print(f"Range da vari√°vel target: {df[target_col].min():.2f} - {df[target_col].max():.2f}")
print(f"M√©dia da vari√°vel target: {df[target_col].mean():.2f}")
print(f"Desvio padr√£o da vari√°vel target: {df[target_col].std():.2f}")

print(f"\nTop 3 vari√°veis mais correlacionadas com {target_col}:")
for i, (col, corr) in enumerate(top_correlations.head(3).items()):
    print(f"  {i+1}. {col}: {corr:.3f}")
```

## üéØ O que Aprendemos com Esta An√°lise?

### **Estrutura dos Dados:**
- Quantas observa√ß√µes e vari√°veis temos
- Quais s√£o os tipos de dados
- Se h√° valores nulos ou problemas

### **Distribui√ß√µes:**
- Como as vari√°veis se distribuem
- Se h√° outliers que podem afetar os modelos
- Se as vari√°veis seguem distribui√ß√µes normais

### **Relacionamentos:**
- Quais vari√°veis est√£o mais correlacionadas com o target
- Se h√° multicolinearidade entre features
- Quais vari√°veis podem ser mais importantes para predi√ß√£o

### **Qualidade dos Dados:**
- Se precisamos tratar outliers
- Se precisamos normalizar vari√°veis
- Se h√° dados inconsistentes

## üîç Pr√≥ximos Passos Ap√≥s a EDA

1. **Pr√©-processamento dos dados** (se necess√°rio)
2. **Divis√£o em treino/teste**
3. **Implementa√ß√£o dos modelos de regress√£o**
4. **Compara√ß√£o de performance**

## üí° Dicas Importantes

- **Sempre** comece com EDA antes de modelar
- **Documente** suas descobertas
- **Visualize** os dados de diferentes formas
- **Questionar** os dados: fazem sentido?
- **Identificar** padr√µes que podem ajudar na modelagem

Esta an√°lise explorat√≥ria √© fundamental para entender seus dados e tomar decis√µes informadas sobre como proceder com a modelagem! 